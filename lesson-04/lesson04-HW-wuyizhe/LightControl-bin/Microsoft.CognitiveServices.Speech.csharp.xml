<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.CognitiveServices.Speech.csharp</name>
    </assembly>
    <members>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.Utf8StringMarshaler">
            <summary>
            UTF-8 string marshaler.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringMarshaler.MarshalNativeToManaged(System.IntPtr)">
            <summary>Converts the unmanaged data to managed data.</summary>
            <param name="native">A pointer to the unmanaged data to be wrapped.</param>
            <returns>An object that represents the managed view of the COM data.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringMarshaler.MarshalManagedToNative(System.Object)">
            <summary>Converts the managed data to unmanaged data.</summary>
            <param name="managed">The managed object to be converted.</param>
            <returns>A pointer to the COM view of the managed object.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.BotConnectorConfig">
            <summary>
            Class that defines configurations for speech bot connector
            Added in version 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.BotConnectorConfig.FromSecretKey(System.String,System.String,System.String)">
            <summary>
            Creates an instance of the bot connector config with the specified Speech Channel secret key.
            </summary>
            <param name="secretKey">Speech Channel secret key.</param>
            <param name="subscription">Subscription key associated with the bot</param>
            <param name="region">The region name (see the <a href="https://aka.ms/csspeech/region">region page</a>).</param>
            <returns>A new bot connector config.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.BotConnectorConfig.SecretKey">
            <summary>
            Speech Channel secret key.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.BotConnectorConfig.BotInitialSilenceTimeout">
            <summary>
            Bot Connection initial silence timeout property.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.BotConnectorConfig.TextToSpeechAudioFormat">
            <summary>
            The format of the audio that is returned during text to speech.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs">
            <summary>
            Class for activity received event arguments.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs.Activity">
            <summary>
            Gets the activity associated with the event as a serialized json.
            </summary>
            <returns>The activity.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs.HasAudio">
            <summary>
            Checks if the event contains audio.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs.Audio">
            <summary>
            Gets the audio associated with the event.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector">
            <summary>
            Connects to a speech enabled bot.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.SessionStarted">
            <summary>
            Signal that indicates the start of a listening session.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.SessionStopped">
            <summary>
            Signal that indicates the end of a listening session.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.Recognized">
            <summary>
            Signal for events containing speech recognition results.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.Recognizing">
            <summary>
            Signal for events containing intermediate recognition results.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.Canceled">
            <summary>
            Signal for events relating to the cancellation of an interaction.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.ActivityReceived">
            <summary>
            Signal that an activity was received from the bot.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.#ctor(Microsoft.CognitiveServices.Speech.Dialog.BotConnectorConfig)">
            <summary>
            Creates a speech bot connector using the default microphone input for a specified bot connector configuration.
            </summary>
            <param name="config">Bot connector config.</param>
            <returns>A speech bot connector instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.#ctor(Microsoft.CognitiveServices.Speech.Dialog.BotConnectorConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a speech bot connector using the specified bot connector and audio configuration.
            </summary>
            <param name="config">Bot connector config.</param>
            <param name="audioConfig">Audio config.</param>
            <returns>A speech bot connector instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.gch">
            <summary>
            GC handle for callbacks for context.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.disposed">
            <summary>
            disposed is a flag used to indicate if object is disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.isDisposing">
            <summary>
            isDisposing is a flag used to indicate if object is being disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.botLock">
            <summary>
            botLock is used to synchronize access to objects member variables from multiple threads
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.FireEvent_SessionStarted(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a SessionStarted C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.FireEvent_SessionStopped(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a SessionStopped C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.FireEvent_Recognizing(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a Recognizing C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.FireEvent_Recognized(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a Recognized C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.FireEvent_Canceled(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a Canceled C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.FireEvent_ActivityReceived(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a ActivityReceived C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.ConnectAsync">
            <summary>
            Connects with the back end.
            </summary>
            <returns>An asynchronous operation that starts the connection.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.DisconnectAsync">
            <summary>
            Disconnects from the back end.
            </summary>
            <returns>An asynchronous operation that starts the disconnection.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.SendActivityAsync(System.String)">
            <summary>
            Sends an activity to the backing bot.
            </summary>
            <param name="activityJSON">Activity to send as a serialized JSON</param>
            <returns>An asynchronous operation that starts the operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Initiates keyword recognition.
            </summary>
            <param name="model">Specifies the keyword model to be used.</param>
            <returns>An asynchronous operation that starts the operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.StopKeywordRecognitionAsync">
            <summary>
            Stop keyword recognition.
            </summary>
            <returns>An asynchronous operation that starts the operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.SpeechBotConnector.ListenOnceAsync">
            <summary>
            Starts a listening session that will terminate after the first utterance.
            </summary>
            <returns>An asynchronous operation that starts the operation.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioConfig">
            <summary>
            Represents audio input configuration used for specifying what type of input to use (microphone, file, stream).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromDefaultMicrophoneInput">
            <summary>
            Creates an AudioConfig object representing the default microphone on the system.
            </summary>
            <returns>The audio input configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromWavFileInput(System.String)">
            <summary>
            Creates an AudioConfig object representing the specified file.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="fileName">Specifies the audio input file.</param>
            <returns>The audio input configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.AudioInputStream)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="audioStream">Specifies the custom audio input stream.</param>
            <returns>The audio input configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="callback">Specifies the pull audio input stream callback.</param>
            <returns>The audio input configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="callback">Specifies the pull audio input stream callback.</param>
            <param name="format">The audio data format in which audio will be written to the push audio stream's write() method.</param>
            <returns>The audio input configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromMicrophoneInput(System.String)">
            <summary>
            Creates an AudioConfig object representing the designated input device.
            NOTE: This method was added in version 1.3.0.
            </summary>
            <param name="deviceName">Specifies the device name. Please refer to <a href="https://aka.ms/csspeech/microphone-selection">this page</a> on how to retrieve platform-specific microphone names.</param>
            <returns>The audio input configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromDefaultSpeakerOutput">
            <summary>
            Creates an AudioConfig object representing the default speaker on the system.
            Added in version 1.4.0
            </summary>
            <returns>The audio output configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromWavFileOutput(System.String)">
            <summary>
            Creates an AudioConfig object representing the specified file.
            Added in version 1.4.0
            </summary>
            <param name="fileName">Specifies the audio output file.</param>
            <returns>The audio output configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamOutput(Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Added in version 1.4.0
            </summary>
            <param name="audioStream">Specifies the custom audio output stream.</param>
            <returns>The audio output configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamOutput(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Added in version 1.4.0
            </summary>
            <param name="callback">Specifies the push audio output stream callback.</param>
            <returns>The audio output configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamOutput(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Added in version 1.4.0
            </summary>
            <param name="callback">Specifies the push audio output stream callback.</param>
            <param name="format">The audio data format in which audio will be sent to the push audio stream's write() method.</param>
            <returns>The audio input configuration being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream">
            <summary>
            Represents audio input stream used for custom audio input configurations.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePushStream">
            <summary>
            Creates a memory backed PushAudioInputStream using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <returns>The push audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePushStream(Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a memory backed PushAudioInputStream with the specified audio format.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="format">The audio data format in which audio will be written to the push audio stream's write() method.</param>
            <returns>The push audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePullStream(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods, using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback</param>
            <returns>The pull audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePullStream(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback.</param>
            <param name="format">The audio data format in which audio will be returned from the callback's read() method.</param>
            <returns>The pull audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.isDisposing">
            <summary>
            isDisposing is a flag used to indicate if object is being disposed.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream">
            <summary>
            Represents memory backed push audio input stream used for custom audio input configurations.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.#ctor">
            <summary>
            Creates a memory backed PushAudioInputStream using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a memory backed PushAudioInputStream with the specified audio format.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="format">The audio data format in which audio will be written to the push audio stream's write() method.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Write(System.Byte[])">
            <summary>
            Writes the audio data specified by making an internal copy of the data.
            Note: The dataBuffer should not contain any audio header.
            </summary>
            <param name="dataBuffer">The audio buffer of which this function will make a copy.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Write(System.Byte[],System.Int32)">
            <summary>
            Writes the audio data specified by making an internal copy of the data.
            </summary>
            <param name="dataBuffer">The audio buffer of which this function will make a copy.</param>
            <param name="size">The size of the data in the audio buffer. Note the size could be smaller than dataBuffer.Length</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Set value of a property associated to data buffer. The properties of the audio data should be set before writing the audio data.
            Added in version 1.5.0
            </summary>
            <param name="id">A property Id.</param>
            <param name="value">The value of the property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.SetProperty(System.String,System.String)">
            <summary>
            Set value of a property associated to data buffer. The properties of the audio data should be set before writing the audio data.
            Added in version 1.5.0
            </summary>
            <param name="name">The name of the property.</param>
            <param name="value">The value of the property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Close">
            <summary>
            Closes the stream.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream">
            <summary>
            Represents audio input stream used for custom audio input configurations.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback.</param>
            <returns>The pull audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback.</param>
            <param name="format">The audio data format in which audio will be returned from the callback's read() method.</param>
            <returns>The pull audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback">
            <summary>
            An abstract base class that defines callback methods (Read() and Close()) for custom audio input streams).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Read(System.Byte[],System.UInt32)">
            <summary>
            Reads binary data from the stream.
            Note: The dataBuffer returned by Read() should not contain any audio header.
            </summary>
            <param name="dataBuffer">The buffer to fill</param>
            <param name="size">The size of the buffer.</param>
            <returns>The number of bytes filled, or 0 in case the stream hits its end and there is no more data available.
            If there is no data immediately available, Read() blocks until the next data becomes available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Get property associated to data buffer, such as a timestamp or userId. if the property is not available, an empty string must be returned.
            Added in version 1.5.0
            </summary>
            <param name="id">A property id.</param>
            <returns>The value of the property </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Close">
            <summary>
            Closes the audio input stream.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream">
            <summary>
            Represents audio output stream used for custom audio output configurations.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.CreatePullStream">
            <summary>
            Creates a memory backed PullAudioOutputStream using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <returns>The pull audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.CreatePullStream(Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a memory backed PullAudioOutputStream with the specified audio format.
            </summary>
            <param name="format">The audio data format in which audio will be read from the pull audio stream's read() method.</param>
            <returns>The pull audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.CreatePushStream(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback)">
            <summary>
            Creates a PushAudioOutputStream that delegates to the specified callback interface for write() and close() methods, using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <param name="callback">The custom audio output object, derived from PushAudioOutputStreamCallback</param>
            <returns>The push audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.CreatePushStream(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a PushAudioOutputStream that delegates to the specified callback interface for write() and close() methods.
            </summary>
            <param name="callback">The custom audio output object, derived from PushAudioOutputStreamCallback.</param>
            <param name="format">The audio data format in which audio will be sent to the callback's write() method.</param>
            <returns>The push audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.isDisposing">
            <summary>
            isDisposing is a flag used to indicate if object is being disposed.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream">
            <summary>
            Represents memory backed pull audio output stream used for custom audio output configurations.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream.#ctor">
            <summary>
            Creates a memory backed PullAudioOutputStream using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a memory backed PullAudioOutputStream with the specified audio format.
            </summary>
            <param name="format">The audio data format in which audio will be read from the pull audio stream's read() method.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream.Read(System.Byte[])">
            <summary>
            Read audio from the stream.
            The maximal number of bytes to be read is determined by the size of dataBuffer.
            If there is no data immediately available, read() blocks until the next data becomes available.
            </summary>
            <param name="buffer">The buffer to receive the audio data</param>
            <returns>The number of bytes filled, or 0 in case the stream hits its end and there is no more data available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStream">
            <summary>
            Represents audio output stream used for custom audio output configurations.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback)">
            <summary>
            Creates a PushAudioOutputStream that delegates to the specified callback interface for write() and close() methods using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <param name="callback">The custom audio output object, derived from PushAudioOutputStreamCallback.</param>
            <returns>The push audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a PushAudioOutputStream that delegates to the specified callback interface for write() and close() methods.
            </summary>
            <param name="callback">The custom audio output object, derived from PushAudioOutputStreamCallback.</param>
            <param name="format">The audio data format in which audio will be sent to the callback's write() method.</param>
            <returns>The push audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback">
            <summary>
            An abstract base class that defines callback methods (Write() and Close()) for custom audio output streams).
            Added in version 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Write(System.Byte[])">
            <summary>
            Writes binary data to the stream.
            </summary>
            <param name="dataBuffer">The buffer containing the data to be writen</param>
            <returns>The number of bytes writen.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Close">
            <summary>
            Closes the audio output stream.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat">
            <summary>
            Supported audio input container formats.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.OGG_OPUS">
            <summary>
            Stream ContainerFormat definition for OGG OPUS.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.MP3">
            <summary>
            Stream ContainerFormat definition for MP3.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.FLAC">
            <summary>
            Stream ContainerFormat definition for FLAC. Not supported yet. 
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat">
            <summary>
            Represents audio stream format used for custom audio input configurations.
            Updated in version 1.5.0.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetDefaultInputFormat">
            <summary>
            Creates an audio stream format object representing the default microphone input format (16 kHz, 16 bit, mono PCM).
            </summary>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetDefaultOutputFormat">
            <summary>
            Creates an audio stream format object representing the default speaker output format (16 kHz, 16 bit, mono PCM).
            Added in version 1.4.0
            </summary>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetWaveFormatPCM(System.UInt32,System.Byte,System.Byte)">
            <summary>
            Creates an audio stream format object with the specified PCM waveformat characteristics.
            Currently, only WAV / PCM with 16-bit samples, 16 kHz sample rate, and a single channel (Mono) is supported. When used with Conversation Transcription, eight channels are supported.
            </summary>
            <param name="samplesPerSecond">Sample rate, in samples per second (Hertz).</param>
            <param name="bitsPerSample">Bits per sample.</param>
            <param name="channels">Number of channels in the waveform-audio data.</param>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetCompressedFormat(Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat)">
            <summary>
            Creates an audio stream format object with the specified compressed audio container format, to be used as input format.
            Support added in 1.4.0.
            </summary>
            <param name="compressedFormat">Formats are defined in AudioStreamContainerFormat enum</param>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.AudioDataStream">
            <summary>
            Represents audio data stream used for operating audio data as a stream.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.FromResult(Microsoft.CognitiveServices.Speech.SpeechSynthesisResult)">
            <summary>
            Creates a memory backed AudioDataStream from given speech synthesis result.
            </summary>
            <param name="result">The speech synthesis result.</param>
            <returns>The audio data stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.GetStatus">
            <summary>
            Get current status of the audio data stream.
            </summary>
            <returns>Current status.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.CanReadData(System.UInt32)">
            <summary>
            Check whether the stream has enough data to be read.
            </summary>
            <param name="bytesRequested">The requested data size in bytes.</param>
            <returns>A bool indicating whether the stream has enough data to be read.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.CanReadData(System.UInt32,System.UInt32)">
            <summary>
            Check whether the stream has enough data to be read, starting from the specified position.
            </summary>
            <param name="pos">The position counting from start of the stream.</param>
            <param name="bytesRequested">The requested data size in bytes.</param>
            <returns>A bool indicating whether the stream has enough data to be read.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.ReadData(System.Byte[])">
            <summary>
            Reads the audio data from the audio data stream.
            The maximal number of bytes to be read is determined by the size of buffer.
            If there is no data immediately available, ReadData() blocks until the next data becomes available.
            </summary>
            <param name="buffer">The buffer to receive the audio data.</param>
            <returns>The number of bytes filled, or 0 in case the stream hits its end and there is no more data available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.ReadData(System.UInt32,System.Byte[])">
            <summary>
            Reads the audio data from the audio data stream, starting from the specified position.
            The maximal number of bytes to be read is determined by the size of buffer.
            If there is no data immediately available, ReadData() blocks until the next data becomes available.
            </summary>
            <param name="pos">The position counting from start of the stream.</param>
            <param name="buffer">The buffer to receive the audio data.</param>
            <returns>The number of bytes filled, or 0 in case the stream hits its end and there is no more data available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.SaveToWaveFileAsync(System.String)">
            <summary>
            Save the audio data to a file, asynchronously.
            </summary>
            <param name="fileName">Name of the file for saving.</param>
            <returns>An asynchronous operation representing the saving.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.SetPosition(System.UInt32)">
            <summary>
            Set current position of the audio data stream.
            </summary>
            <param name="pos">Position to be set.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.GetPosition">
            <summary>
            Get current position of the audio data stream.
            </summary>
            <returns>Current position.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.AudioDataStream.Properties">
            <summary>
            Contains properties of the audio data stream.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Connection">
            <summary>
            Connection is a proxy class for managing connection to the speech service of the specified Recognizer.
            By default, a Recognizer autonomously manages connection to service when needed. 
            The Connection class provides additional methods for users to explicitly open or close a connection and 
            to subscribe to connection status changes.
            The use of Connection is optional, and mainly for scenarios where fine tuning of application
            behavior based on connection status is needed. Users can optionally call Open() to manually set up a connection 
            in advance before starting recognition on the Recognizer associated with this Connection. After starting recognition,
            calling Open() or Close() might fail, depending on the process state of the Recognizer. But this does not affect 
            the state of the associated Recognizer. And if the Recognizer needs to connect or disconnect to service, it will 
            setup or shutdown the connection independently. In this case the Connection will be notified by change of connection 
            status via Connected/Disconnected events.
            Added in version 1.2.0.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.FromRecognizer(Microsoft.CognitiveServices.Speech.Recognizer)">
            <summary>
            Gets the Connection instance from the specified recognizer. 
            </summary>
            <param name="recognizer">The recognizer associated with the connection.</param>
            <returns>The Connection instance of the recognizer.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.Open(System.Boolean)">
            <summary>
            Starts to set up connection to the service.
            Users can optionally call Open() to manually set up a connection in advance before starting recognition on the 
            Recognizer associated with this Connection. After starting recognition, calling Open() might fail, depending on 
            the process state of the Recognizer. But the failure does not affect the state of the associated Recognizer.
            Note: On return, the connection might not be ready yet. Please subscribe to the Connected event to
            be notfied when the connection is established.
            </summary>
            <param name="forContinuousRecognition">Indicates whether the connection is used for continuous recognition or single-shot recognition.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.Close">
            <summary>
            Closes the connection the service.
            Users can optionally call Close() to manually shutdown the connection of the associated Recognizer. The call
            might fail, depending on the process state of the Recognizer. But the failure does not affect the state of the 
            associated Recognizer.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Connection.Connected">
            <summary>
            The Connected event to indicate that the recognizer is connected to service.
            In order to receive the Connected event after subscribing to it, the Connection object itself needs to be alive.
            If the Connection object owning this event is out of its life time, all subscribed events won't be delivered.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Connection.Disconnected">
            <summary>
            The Diconnected event to indicate that the recognizer is disconnected from service.
            In order to receive the Disconnected event after subscribing to it, the Connection object itself needs to be alive.
            If the Connection object owning this event is out of its life time, all subscribed events won't be delivered.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.FireEvent_Connected(System.IntPtr,System.IntPtr)">
             <summary>
             Defines a private methods which raise a C# event when a corresponding callback is invoked from the native layer.
             </summary>
            
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ConnectionEventArgs">
            <summary>
            Defines payload for Connected/Disconnected events
            Added in version 1.2.0.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ConnectionEventType">
            <summary>
            Define connection event types.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber">
            <summary>
            Perform conversation transcribing on the speech input. It returns recognized text and speaker id.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.Recognizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.Recognizing"/> signals that an intermediate recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.Canceled"/> signals that the speech recognition was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of ConversationTranscriber.
            </summary>
            <param name="speechConfig">Speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of ConversationTranscriber.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.ConversationId">
            <summary>
            Gets/sets the conversation id.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.AuthorizationToken">
            <summary>
            Gets/sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.Properties">
            <summary>
            Gets the collection or properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber"/>.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.StartTranscribingAsync">
            <summary>
            Starts conversation trancsribing on a continuous audio stream, until StopTranscribingAsync() is called.
            User must subscribe to events to receive recognition results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.StopTranscribingAsync">
            <summary>
            Stops conversation transcribing.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
            <remarks>This is used to pause the conversation. The client can start the conversation again by calling StartTranscribingAsync.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.EndConversationAsync">
             <summary>
             End a conversation.
             </summary>
            
             <returns>A task representing the asynchronous operation that ends the recognition.</returns>
             <remarks>This is used to communicate to the service to shutdown the conversation in the service.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.AddParticipant(System.String)">
             <summary>
             Add a participant to a conversation using the user's id.
            
             Note: The returned participants can be used to remove. If the client changes the participant's attributes,
             the changed attributes are passed on to the service only when the participants is added again.
            
             </summary>
             <param name="userId">A user id.</param>
             <returns>A Participant object.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.AddParticipant(Microsoft.CognitiveServices.Speech.Conversation.User)">
            <summary>
            Add a participant to a conversation using the User object.
            </summary>
            <param name="user">A User object.</param>
            <returns>void.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.AddParticipant(Microsoft.CognitiveServices.Speech.Conversation.Participant)">
            <summary>
            Add a participant to a conversation using the Participant object
            </summary>
            <param name="participant">A Participant object.</param>
            <returns>void.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.RemoveParticipant(Microsoft.CognitiveServices.Speech.Conversation.Participant)">
            <summary>
            Remove a participant in a conversation using the Participant object
            </summary>
            <param name="participant">A Participant object.</param>
            <returns>void.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.RemoveParticipant(Microsoft.CognitiveServices.Speech.Conversation.User)">
            <summary>
            Remove a participant in a conversation using the User object
            </summary>
            <param name="user">A User object.</param>
            <returns>void.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriber.RemoveParticipant(System.String)">
            <summary>
            Remove a participant from a conversation using a user id object
            </summary>
            <param name="userId">A user id.</param>
            <returns>void.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionResult">
            <summary>
            Class that defines the result of conversation transcriber.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionResult.UserId">
            <summary>
            A string that represents the user id.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionResult.ToString">
            <summary>
            Returns a string that represents the conversation transcription result.
            </summary>
            <returns>A string that represents the conversation transcription result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionEventArgs">
            <summary>
            Class that defines conversation transcriber event arguments.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionEventArgs.Result">
            <summary>
            Represents the conversation transcription result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the conversation transcription result event.
            </summary>
            <returns>A string that represents the conversation transcription event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionCanceledEventArgs">
            <summary>
            Defines payload of conversation transcriber canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionCanceledEventArgs.Reason">
            <summary>
            The reason the result was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.ConversationTranscriptionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the conversation transcription result event.
            </summary>
            <returns>A string that represents the conversation transcriber event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Conversation.Participant">
            <summary>
            Represents a participant in a conversation.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.Participant.From(System.String,System.String,System.String)">
            <summary>
            Creates a Participant using user id, her/his preferred language and her/his voice signature.
            If voice signature is empty then user will not be identified.
            </summary>
            <param name="userId">A user id.</param>
            <param name="preferredLanguage">A preferred language.</param>
            <param name="voiceSignature">A voice signature of the user.</param>
            <returns>A Participant object</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.Participant.#ctor(System.IntPtr)">
            <summary>
            Internal constructor
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.Participant.PreferredLanguage">
            <summary>
            The participant's preferred spoken language.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.Participant.VoiceSignature">
            <summary>
            The participant's voice signature.
            If voice signature is empty then user will not be identified.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.Participant.Properties">
            <summary>
            Contains properties of the participant.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Conversation.User">
            <summary>
            Represents a user in a conversation.
            Added in version 1.5.0 
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.User.FromUserId(System.String)">
            <summary>
            Create a user using user id
            </summary>
            <param name="userId">A user id.</param>
            <returns>A user object</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Conversation.User.#ctor(System.IntPtr)">
            <summary>
            Internal constructor
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Conversation.User.UserId">
            <summary>
            Get user id.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResultCollection">
            <summary>
            Collection of best recognitions.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResultCollection.NBest">
            <summary>
            Enumerable of alternative interpretations of the same speech recognition result.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult">
            <summary>
            Detailed recognition result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.Confidence">
            <summary>
            Confidence of recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.Text">
            <summary>
            Recognized text.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.LexicalForm">
            <summary>
            Raw lexical form.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.NormalizedForm">
            <summary>
            Normalized form.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.MaskedNormalizedForm">
            <summary>
            Normalized form with masked profanity.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel">
            <summary>
            Represents language understanding model used for intent recognition.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel.FromEndpoint(System.String)">
            <summary>
            Creates an language understanding model using the specified endpoint.
            </summary>
            <param name="uri">A string that represents the endpoint of the language understanding model.</param>
            <returns>The language understanding model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel.FromAppId(System.String)">
            <summary>
            Creates an language understanding model using the application id of Language Understanding service.
            </summary>
            <param name="appId">A string that represents the application id of Language Understanding service.</param>
            <returns>The language understanding model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel.FromSubscription(System.String,System.String,System.String)">
            <summary>
            Creates an language understanding model using hostname, subscription key and application id of Language Understanding service.
            </summary>
            <param name="subscriptionKey">A string that represents the subscription key of Language Understanding service.</param>
            <param name="appId">A string that represents the application id of Language Understanding service.</param>
            <param name="region">A string that represents the region of the Language Understanding service (see the <a href="https://aka.ms/csspeech/region">region page</a>).</param>
            <returns>The language understanding model being created.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult">
            <summary>
            Defines result of intent recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.IntentId">
            <summary>
            A string that represents the intent identifier being recognized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.ToString">
            <summary>
            Returns a string that represents the intent recognition result.
            </summary>
            <returns>A string that represents the intent recognition result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionEventArgs">
            <summary>
            Define payload of intent recognizing/recognized events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionEventArgs.Result">
            <summary>
            Represents the intent recognition result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the intent recognition result event.
            </summary>
            <returns>A string that represents the intent recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs">
            <summary>
            Define payload of intent recognition canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.Reason">
            <summary>
            The reason the result was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in version 1.1.0.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the intent recognition result event.
            </summary>
            <returns>A string that represents the intent recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer">
            <summary>
            Perform intent recognition on the speech input. It returns both recognized text and recognized intent.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognizing"/> signals that an intermediate recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Canceled"/> signals that the intent recognition was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of IntentRecognizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of IntentRecognizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AuthorizationToken">
            <summary>
            Gets/sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Properties">
            <summary>
            Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer"/>.
            Note: The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.RecognizeOnceAsync">
            <summary>
            Starts speech recognition with intent recognition, and returns after a single utterance is recognized. The end of a
            single utterance is determined by listening for silence at the end or until a maximum of 15
            seconds of audio is processed.  The task returns the recognition text as result.
            Note: Since RecognizeOnceAsync() returns only a single utterance, it is suitable only for single
            shot recognition like command or query.
            For long-running multi-utterance recognition, use StartContinuousRecognitionAsync() instead.
            </summary>
            <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult"/></returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StartContinuousRecognitionAsync">
            <summary>
            Starts speech recognition on a continuous audio stream, until StopContinuousRecognitionAsync() is called.
            User must subscribe to events to receive recognition results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StopContinuousRecognitionAsync">
            <summary>
            Stops continuous intent recognition.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Starts speech recognition on a continuous audio stream with keyword spotting, until StopKeywordRecognitionAsync() is called.
            User must subscribe to events to receive recognition results.
            </summary>
            Note: Keyword spotting (KWS) functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.
            <param name="model">The keyword recognition model that specifies the keyword to be recognized.</param>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StopKeywordRecognitionAsync">
            <summary>
            Stops continuous speech recognition with keyword spotting.
            </summary>
            Note: Keyword spotting (KWS) functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(System.String)">
            <summary>
            Adds a simple phrase that may be spoken by the user, indicating a specific user intent.
            </summary>
            <param name="simplePhrase">The phrase corresponding to the intent.</param>
            <remarks>Once recognized, the IntentRecognitionResult's IntentId property will match the simplePhrase specified here.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(System.String,System.String)">
            <summary>
            Adds a simple phrase that may be spoken by the user, indicating a specific user intent.
            </summary>
            <param name="simplePhrase">The phrase corresponding to the intent.</param>
            <param name="intentId">A custom id string to be returned in the IntentRecognitionResult's IntentId property.</param>
            <remarks>Once recognized, the result's intent id will match the id supplied here.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel,System.String)">
            <summary>
            Adds a single intent by name from the specified Language Understanding Model.
            </summary>
            <param name="model">The language understanding model containing the intent.</param>
            <param name="intentName">The name of the single intent to be included from the language understanding model.</param>
            <remarks>Once recognized, the IntentRecognitionResult's IntentId property will contain the intentName specified here.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel,System.String,System.String)">
            <summary>
            Adds a single intent by name from the specified Language Understanding Model.
            </summary>
            <param name="model">The language understanding model containing the intent.</param>
            <param name="intentName">The name of the single intent to be included from the language understanding model.</param>
            <param name="intentId">A custom id string to be returned in the IntentRecognitionResult's IntentId property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddAllIntents(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel,System.String)">
            <summary>
            Adds all intents from the specified Language Understanding Model.
            </summary>
            <param name="model">The language understanding model from Language Understanding service.</param>
            <param name="intentId">A custom string id to be returned in the IntentRecognitionResult's IntentId property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddAllIntents(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel)">
            <summary>
            Adds all intents from the specified Language Understanding Model.
            </summary>
            <param name="model">The language understanding model from Language Understanding service.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel">
            <summary>
            Represents keyword recognition model used with StartKeywordRecognitionAsync.
            </summary>
            Note: Keyword spotting (KWS) functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel.FromFile(System.String)">
            <summary>
            Creates a keyword recognition model using the specified file.
            </summary>
            <param name="fileName">A string that represents file name for the keyword recognition model.</param>
            <returns>The keyword recognition model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.NoMatchReason">
            <summary>
            Defines the possible reasons a recognition result might not be recognized.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.NotRecognized">
            <summary>
            Indicates that speech was detected, but not recognized.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.InitialSilenceTimeout">
            <summary>
            Indicates that the start of the audio stream contained only silence, and the service timed out waiting for speech.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.InitialBabbleTimeout">
            <summary>
            Indicates that the start of the audio stream contained only noise, and the service timed out waiting for speech.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.KeywordNotRecognized">
            <summary>
            Indicates that the spotted keyword has been rejected by the keyword verification service.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.OutputFormat">
            <summary>
            Output format.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ProfanityOption">
            <summary>
            Profanity option.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PropertyCollection">
            <summary>
            Class to retrieve or set a property value from a property collection.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Returns value of a property.
            If the property value is not defined, an empty string is returned.
            </summary>
            <param name="id">The ID of property. See <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId"/></param>
            <returns>value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(System.String)">
            <summary>
            Returns value of a property.
            If the property value is not defined, an empty string is returned.
            </summary>
            <param name="propertyName">The name of property</param>
            <returns>value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Returns value of a property.
            If the property value is not defined, the specified default value is returned.
            </summary>
            <param name="id">The id of property. See <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId"/></param>
            <param name="defaultValue">The default value which is returned if no value is defined for the property.</param>
            <returns>value of the property.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(System.String,System.String)">
            <summary>
            Returns value of a property.
            If the property value is not defined, the specified default value is returned.
            </summary>
            <param name="propertyName">The name of property.</param>
            <param name="defaultValue">The default value which is returned if no value is defined for the property.</param>
            <returns>value of the property.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Set value of a property.
            </summary>
            <param name="id">The id of property. See <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId"/></param>
            <param name="value">value to set</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.SetProperty(System.String,System.String)">
            <summary>
            Set value of a property.
            </summary>
            <param name="propertyName">The name of property.</param>
            <param name="value">value to set</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PropertyId">
            <summary>
            Defines speech property ids.
            Changed in version 1.4.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Key">
            <summary>
            The Cognitive Services Speech Service subscription key. If you are using an intent recognizer, you need
            to specify the LUIS endpoint key for your particular LUIS app. Under normal circumstances, you shouldn't
            have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromSubscription(System.String,System.String)"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Endpoint">
            <summary>
            The Cognitive Services Speech Service endpoint (url). Under normal circumstances, you shouldn't
            have to use this property directly.
            Instead, use  <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)"/>, or <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)"/>.
            NOTE: This endpoint is not the same as the endpoint used to obtain an access token.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Region">
            <summary>
            The Cognitive Services Speech Service region. Under normal circumstances, you shouldn't have to
            use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromSubscription(System.String,System.String)"/>, <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)"/>,
            <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)"/>, <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromAuthorizationToken(System.String,System.String)"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceAuthorization_Token">
            <summary>
            The Cognitive Services Speech Service authorization token (aka access token). Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromAuthorizationToken(System.String,System.String)"/>,
            <see cref="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.AuthorizationToken"/>, <see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AuthorizationToken"/>, <see cref="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.AuthorizationToken"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceAuthorization_Type">
            <summary>
            The Cognitive Services Speech Service authorization type. Currently unused.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_EndpointId">
            <summary>
            The Cognitive Services Custom Speech Service endpoint id. Under normal circumstances, you shouldn't
            have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)"/>, or <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)"/>.
            NOTE: The endpoint id is available in the Custom Speech Portal, listed under Endpoint Details.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyHostName">
            <summary>
            The host name of the proxy server used to connect to the Cognitive Services Speech Service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            NOTE: This property id was added in version 1.1.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyPort">
            <summary>
            The port of the proxy server used to connect to the Cognitive Services Speech Service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            NOTE: This property id was added in version 1.1.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyUserName">
            <summary>
            The user name of the proxy server used to connect to the Cognitive Services Speech Service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            NOTE: This property id was added in version 1.1.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyPassword">
            <summary>
            The password of the proxy server used to connect to the Cognitive Services Speech Service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            NOTE: This property id was added in version 1.1.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Url">
            <summary>
            The URL string built from speech configuration.
            This property is intended to be read-only. The SDK is using it internally.
            NOTE: Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_TranslationToLanguages">
            <summary>
            The list of comma separated languages (in BCP-47 format) used as target translation languages. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.AddTargetLanguage(System.String)"/> and the read-only <see cref="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.TargetLanguages"/> collection.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_TranslationVoice">
            <summary>
            The name of the Cognitive Service Text to Speech Service voice. Under normal circumstances, you shouldn't have to use this
            property directly. Instead use <see cref="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.VoiceName"/>.
            NOTE: Valid voice names can be found <a href="https://aka.ms/csspeech/voicenames">here</a>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_TranslationFeatures">
            <summary>
            Translation features. For internal use.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_IntentRegion">
            <summary>
            The Language Understanding Service Region. Under normal circumstances, you shouldn't have to use this property directly.
            Instead use <see cref="T:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_RecoMode">
            <summary>
            The Cognitive Services Speech Service recognition mode. Can be "INTERACTIVE", "CONVERSATION", "DICTATION".
            This property is intended to be read-only. The SDK is using it internally.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_RecoLanguage">
            <summary>
            The spoken language to be recognized (in BCP-47 format). Under normal circumstances, you shouldn't have to use this property directly.
            Instead, use <see cref="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechRecognitionLanguage"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Speech_SessionId">
            <summary>
            The session id. This id is a universally unique identifier (aka UUID) representing a specific binding of an audio input stream
            and the underlying speech recognition instance to which it is bound. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="P:Microsoft.CognitiveServices.Speech.SessionEventArgs.SessionId"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthLanguage">
            <summary>
            The spoken language to be synthesized (e.g. en-US)
            Added in version 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthVoice">
            <summary>
            The name of the voice to be used for speech synthesis
            Added in version 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthOutputFormat">
            <summary>
            The string to specify speech synthesis output audio format (e.g. riff-16khz-16bit-mono-pcm)
            Added in version 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs">
            <summary>
            The initial silence timeout value (in milliseconds) used by the service.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs">
            <summary>
            The end silence timeout value (in milliseconds) used by the service.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_EnableAudioLogging">
            <summary>
            A boolean value specifying whether audio logging is enabled in the service or not.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestDetailedResultTrueFalse">
            <summary>
            The requested Cognitive Services Speech Service response output format (simple or detailed). Under normal circumstances, you shouldn't have
            to use this property directly.
            Instead, use <see cref="P:Microsoft.CognitiveServices.Speech.SpeechConfig.OutputFormat"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestProfanityFilterTrueFalse">
            <summary>
            The requested Cognitive Services Speech Service response output profanity level. Currently unused.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_ProfanityOption">
            <summary>
            The requested Cognitive Services Speech Service response output profanity setting.
            Allowed values are "masked", "removed", and "raw".
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_PostProcessingOption">
            <summary>
            A string value specifying which post processing option should be used by service.
            Allowed values are "TrueText".
            Added in version 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestWordLevelTimestamps">
            <summary>
            A boolean value specifying whether to include word-level timestamps in the response result.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_StablePartialResultThreshold">
            <summary>
            The number of times a word has to be in partial results to be returned.
            Added in version 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_OutputFormatOption">
            <summary>
            A string value specifying the output format option in the response result. Internal use only.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_TranslationRequestStablePartialResult">
            <summary>
            A boolean value to request for stabilizing translation partial results by omitting words in the end.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_JsonResult">
            <summary>
            The Cognitive Services Speech Service response output (in JSON format). This property is available on
            recognition result objects only.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_JsonErrorDetails">
            <summary>
            The Cognitive Services Speech Service error details (in JSON format). Under normal circumstances, you shouldn't have to
            use this property directly. Instead use <see cref="P:Microsoft.CognitiveServices.Speech.CancellationDetails.ErrorDetails"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RecognitionLatencyMs">
            <summary>
            The recognition latency in milliseconds. Read-only, available on final speech/translation/intent results.
            This measures the latency between when an audio input is received by the SDK, and the moment the final result is received from the service.
            The SDK computes the time difference between the last audio fragment from the audio input that is contributing to the final result, and the time the final result is received from the speech service.
            Added in version 1.3.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.CancellationDetails_Reason">
            <summary>
            The cancellation reason. Currently unused.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.CancellationDetails_ReasonText">
            <summary>
            The cancellation text. Currently unused.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.CancellationDetails_ReasonDetailedText">
            <summary>
            The cancellation detailed text. Currently unused.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.LanguageUnderstandingServiceResponse_JsonResult">
            <summary>
            The Language Understanding Service response output (in JSON format). Available via <see cref="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Properties"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Speech_LogFilename">
            <summary>
            The file name to write logs.
            Added in version 1.4.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Secret_Key">
            <summary>
            Speech Channel secret key.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Initial_Silence_Timeout">
            <summary>
            Silence timeout for listening
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_From_Id">
            <summary>
            From Id to add to speech recognition activities.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.ConversationTranscribingService_DataBufferUserId">
            <summary>
            The user id associated to data buffer written by client when using Pull/Push audio mode streams.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.ConversationTranscribingService_DataBufferTimeStamp">
            <summary>
            The time stamp associated to data buffer written by client when using Pull/Push audio mode streams.
            The time stamp is a 64-bit value with a resolution of 90 kHz. The same as the presentation timestamp in an MPEG transport stream.
            See https://en.wikipedia.org/wiki/Presentation_timestamp.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ResultReason">
            <summary>
            Defines the possible reasons a recognition result might be generated.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.NoMatch">
            <summary>
            Indicates speech could not be recognized. More details can be found in the NoMatchDetails object.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.Canceled">
            <summary>
            Indicates that the recognition was canceled. More details can be found using the CancellationDetails object.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizingSpeech">
            <summary>
            Indicates the speech result contains hypothesis text.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedSpeech">
            <summary>
            Indicates the speech result contains final text that has been recognized.
            Speech Recognition is now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizingIntent">
            <summary>
            Indicates the intent result contains hypothesis text and intent.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedIntent">
            <summary>
            Indicates the intent result contains final text and intent.
            Speech Recognition and Intent determination are now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatingSpeech">
            <summary>
            Indicates the translation result contains hypothesis text and its translation(s).
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatedSpeech">
            <summary>
            Indicates the translation result contains final text and corresponding translation(s).
            Speech Recognition and Translation are now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.SynthesizingAudio">
            <summary>
            Indicates the synthesized audio result contains a non-zero amount of audio data
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.SynthesizingAudioCompleted">
            <summary>
            Indicates the synthesized audio is now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizingKeyword">
            <summary>
            Indicates the speech result contains (unverified) keyword text.
            Added in version 1.3.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedKeyword">
            <summary>
            Indicates that keyword recognition completed recognizing the given keyword.
            Added in version 1.3.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.SynthesizingAudioStarted">
            <summary>
            Indicates the speech synthesis is now started.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.CancellationReason">
            <summary>
            Defines the possible reasons a recognition result might be canceled.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationReason.Error">
            <summary>
            Indicates that an error occurred during speech recognition.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationReason.EndOfStream">
            <summary>
            Indicates that the end of the audio stream was reached.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.CancellationErrorCode">
            <summary>
            Defines error code in case that CancellationReason is Error.
            Added in version 1.1.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.NoError">
            <summary>
            No error.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.AuthenticationFailure">
            <summary>
            Indicates an authentication error.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.BadRequest">
            <summary>
            Indicates that one or more recognition parameters are invalid or the audio format is not supported.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.TooManyRequests">
            <summary>
            Indicates that the number of parallel requests exceeded the number of allowed concurrent transcriptions for the subscription.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.Forbidden">
            <summary>
            Indicates that the free subscription used by the request ran out of quota.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ConnectionFailure">
            <summary>
            Indicates a connection error.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ServiceTimeout">
            <summary>
            Indicates a time-out error when waiting for response from service.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ServiceError">
            <summary>
            Indicates that an error is returned by the service.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ServiceUnavailable">
            <summary>
            Indicates that the service is currently unavailable.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.RuntimeError">
            <summary>
            Indicates an unexpected runtime error.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.RecognitionEventArgs">
            <summary>
            Defines payload for recognition events like Speech Start/End Detected
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionEventArgs.Offset">
            <summary>
            Represents the message offset in tick (100 nanoseconds)
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.RecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the recognition event payload.
            </summary>
            <returns>A string that represents the recognition event payload.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.RecognitionEventType">
            <summary>
            Define recognition event types.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.RecognitionResult">
            <summary>
            Contains detailed information about result of a recognition operation.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.ResultId">
            <summary>
            Specifies the result identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Reason">
            <summary>
            Specifies status of speech recognition result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Text">
            <summary>
            Presents the recognized text in the result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Duration">
            <summary>
            Duration of the recognized speech.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.OffsetInTicks">
            <summary>
            Offset of the recognized speech in ticks. A single tick represents one hundred nanoseconds or one ten-millionth of a second.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.RecognitionResult.ToString">
            <summary>
            Returns a string that represents the speech recognition result.
            </summary>
            <returns>A string that represents the speech recognition result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.CancellationDetails">
            <summary>
            Contains detailed information about why a result was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.CancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.RecognitionResult)">
            <summary>
            Creates an instance of CancellationDetails object for the canceled SpeechRecognitionResult.
            </summary>
            <param name="result">The result that was canceled.</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.CancellationDetails.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.CancellationDetails.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.CancellationDetails.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in version 1.1.0.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.CancellationDetails.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.CancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.CancellationDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.NoMatchDetails">
            <summary>
            Contains detailed information for NoMatch recognition results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.NoMatchDetails.FromResult(Microsoft.CognitiveServices.Speech.RecognitionResult)">
            <summary>
            Creates an instance of NoMatchDetails object for NoMatch RecognitionResults.
            </summary>
            <param name="result">The recognition result that was not recognized.</param>
            <returns>The NoMatchDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.NoMatchDetails.Reason">
            <summary>
            The reason the result was not recognized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.NoMatchDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Recognizer">
            <summary>
            Defines the base class Recognizer which mainly contains common event handlers.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SessionStarted">
            <summary>
            Defines event handler for session started event.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SessionStopped">
            <summary>
            Defines event handler for session stopped event.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SpeechStartDetected">
            <summary>
            Defines event handler for speech start detected event.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SpeechEndDetected">
            <summary>
            Defines event handler for speech end detected event.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.gch">
            <summary>
            GC handle for callbacks for context.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.disposed">
            <summary>
            disposed is a flag used to indicate if object is disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.isDisposing">
            <summary>
            isDisposing is a flag used to indicate if object is being disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.recognizerLock">
            <summary>
            recognizerLock is used to synchronize access to objects member variables from multiple threads
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.FireEvent_SetSessionStarted(System.IntPtr,System.IntPtr,System.IntPtr)">
             <summary>
             Define a private methods which raise a C# event when a corresponding callback is invoked from the native layer.
             </summary>
            
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.DoAsyncRecognitionAction(System.Action)">
            <summary>
            This methods checks if a recognizer is disposed before performing async recognition action.
            The Action parameter <paramref name="recoImplAction"/> can be any internal async recognition method of Speech, Translation and Intent Recognizer.
            The method is called from all async recognition methods (e.g. <see cref="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StartContinuousRecognitionAsync"/>).
            ObjectDisposedException will be thrown and the action will not be performed if its recognizer is not available anymore.
            The purpose of this method is to prevent possible race condition if async recognitions are not awaited.
            </summary>
            <param name="recoImplAction">Actual implementation.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ServicePropertyChannel">
            <summary>
            Defines channels used to pass property settings to service.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ServicePropertyChannel.UriQueryParameter">
            <summary>
            Uses URI query parameter to pass property settings to service.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SessionEventArgs">
            <summary>
            Defines payload for SessionStarted/Stopped events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SessionEventArgs.SessionId">
            <summary>
            Represents the session identifier.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SessionEventArgs.ToString">
            <summary>
            Returns a string that represents the session event.
            </summary>
            <returns>A string that represents the session event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SessionEventType">
            <summary>
            Define session event types.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Grammar">
            <summary>
            Represents base class grammar for customizing speech recognition.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Grammar.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hgrammar">Grammar handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Grammar.NativeHandle">
            <summary>
            Internal native handle property.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.GrammarPhrase">
            <summary>
            Represents a phrase that may be spoken by the user.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarPhrase.From(System.String)">
            <summary>
            Creates a grammar phrase using the specified phrase text.
            </summary>
            <param name="text">The text representing a phrase that may be spoken by the user.</param>
            <returns>A shared pointer to a grammar phrase.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarPhrase.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hphrase">Grammar phrase handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.GrammarPhrase.NativeHandle">
            <summary>
            Internal native handle property.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PhraseListGrammar">
            <summary>
            Represents a phrase list grammar for dynamic grammar scenarios.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.FromRecognizer(Microsoft.CognitiveServices.Speech.Recognizer)">
            <summary>
            Creates a phrase list grammar for the specified recognizer.
            </summary>
            <param name="recognizer">The recognizer from which to obtain the phrase list grammar.</param>
            <returns>The phrase list grammar.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hgrammar">Grammar handle.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.AddPhrase(System.String)">
            <summary>
            Adds a simple phrase that may be spoken by the user.
            </summary>
            <param name="text">The phrase to be added.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.Clear">
            <summary>
            Clears all phrases from the phrase list grammar.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.FromRecognizer(Microsoft.CognitiveServices.Speech.Recognizer,System.String)">
            <summary>
            Internal. Creates a phrase list grammar for the specified recognizer, with the specified name.
            </summary>
            <param name="recognizer">The recognizer from which to obtain the phrase list grammar.</param>
            <param name="name">The name of the phrase list grammar to create.</param>
            <returns>The phrase list grammar.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechConfig">
            <summary>
            Speech configuration.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromSubscription(System.String,System.String)">
            <summary>
            Creates an instance of speech configuration with specified subscription key and region.
            </summary>
            <param name="subscriptionKey">The subscription key, can be empty if authorization token is specified later.</param>
            <param name="region">The region name (see the <a href="https://aka.ms/csspeech/region">region page</a>).</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromAuthorizationToken(System.String,System.String)">
            <summary>
            Creates an instance of the speech config with specified authorization token and region.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            As configuration values are copied when creating a new recognizer, the new token value will not apply to recognizers that have already been created.
            For recognizers that have been created before, you need to set authorization token of the corresponding recognizer
            to refresh the token. Otherwise, the recognizers will encounter errors during recognition.
            </summary>
            <param name="authorizationToken">The authorization token.</param>
            <param name="region">The region name (see the <a href="https://aka.ms/csspeech/region">region page</a>).</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)">
            <summary>
            Creates an instance of the speech config with specified endpoint and subscription key.
            This method is intended only for users who use a non-standard service endpoint or parameters.
            Note: The query parameters specified in the endpoint URI are not changed, even if they are set by any other APIs.
            For example, if the recognition language is defined in URI as query parameter "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US",
            the language setting in URI takes precedence, and the effective language is "de-DE".
            Only the parameters that are not specified in the endpoint URI can be set by other APIs.
            Note: To use an authorization token with FromEndpoint, use FromEndpoint(System.Uri),
            and then set the AuthorizationToken property on the created SpeechConfig instance.
            </summary>
            <param name="endpoint">The service endpoint to connect to.</param>
            <param name="subscriptionKey">The subscription key.</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)">
            <summary>
            Creates an instance of the speech config with specified endpoint.
            This method is intended only for users who use a non-standard service endpoint or parameters.
            Note: The query parameters specified in the endpoint URI are not changed, even if they are set by any other APIs.
            For example, if the recognition language is defined in URI as query parameter "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US",
            the language setting in URI takes precedence, and the effective language is "de-DE".
            Only the parameters that are not specified in the endpoint URI can be set by other APIs.
            Note: If the endpoint requires a subscription key for authentication, use FromEndpoint(System.Uri, string) to pass
            the subscription key as parameter.
            To use an authorization token with FromEndpoint, use this method to create a SpeechConfig instance, and then
            set the AuthorizationToken property on the created SpeechConfig instance.
            Note: Added in version 1.5.0.
            </summary>
            <param name="endpoint">The service endpoint to connect to.</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SubscriptionKey">
            <summary>
            Subscription key.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.Region">
            <summary>
            Region.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.AuthorizationToken">
            <summary>
            Gets/sets the authorization token.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            As configuration values are copied when creating a new recognizer, the new token value will not apply to recognizers that have already been created.
            For recognizers that have been created before, you need to set authorization token of the corresponding recognizer
            to refresh the token. Otherwise, the recognizers will encounter errors during recognition.
            Changed in version 1.3.0.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechRecognitionLanguage">
            <summary>
            Specifies the name of spoken language to be recognized in BCP-47 format
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.OutputFormat">
            <summary>
            Output format: simple or detailed.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.EndpointId">
            <summary>
            Sets the endpoint ID of a customized speech model that is used for speech recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechSynthesisLanguage">
            <summary>
            Gets/sets the speech synthesis language, e.g. en-US
            Added in version 1.4.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechSynthesisVoiceName">
            <summary>
            Gets/sets the speech synthesis voice name
            Added in version 1.4.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechSynthesisOutputFormat">
            <summary>
            Gets the speech synthesis output format
            Added in version 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetSpeechSynthesisOutputFormat(Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat)">
            <summary>
            Sets the speech synthesis output format.
            Added in version 1.4.0
            </summary>
            <param name="format">The synthesis output format ID (e.g. Riff16Khz16BitMonoPcm).</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)">
             <summary>
             Sets proxy configuration.
             Added in version 1.1.0
            
             Note: Proxy functionality is not available on macOS. This function will have no effect on this platform.
             </summary>
             <param name="proxyHostName">The host name of the proxy server, without the protocol scheme (http://)</param>
             <param name="proxyPort">The port number of the proxy server.</param>
             <param name="proxyUserName">The user name of the proxy server.</param>
             <param name="proxyPassword">The password of the proxy server.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32)">
            <summary>
            Sets proxy configuration.
            Added in version 1.3.0
            </summary>
            <param name="proxyHostName">The host name of the proxy server.</param>
            <param name="proxyPort">The port number of the proxy server.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProperty(System.String,System.String)">
            <summary>
            Sets the property by name.
            </summary>
            <param name="name">Name of the property</param>
            <param name="value">Value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Sets the property by propertyId
            Added in version 1.3.0.
            </summary>
            <param name="id">PropertyId of the property</param>
            <param name="value">Value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.GetProperty(System.String)">
            <summary>
            Gets the property by name.
            </summary>
            <param name="name">Name of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Gets the property by propertyId
            Added in version 1.3.0.
            </summary>
            <param name="id">PropertyId of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetServiceProperty(System.String,System.String,Microsoft.CognitiveServices.Speech.ServicePropertyChannel)">
            <summary>
            Sets a property value that will be passed to service using the specified channel.
            Added in version 1.5.0.
            </summary>
            <param name="name">The property name.</param>
            <param name="value">The property value.</param>
            <param name="channel">The channel used to pass the specified property to service.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProfanity(Microsoft.CognitiveServices.Speech.ProfanityOption)">
            <summary>
            Sets profanity option.
            Added in version 1.5.0.
            </summary>
            <param name="profanity">The property option to set.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.EnableAudioLogging">
            <summary>
            Enable audio logging in service.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.RequestWordLevelTimestamps">
            <summary>
            Includes word-level timestamps.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.EnableDictation">
            <summary>
            Enable dictation. Only supported in speech continuous recognition.
            Added in version 1.5.0.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResult">
            <summary>
            Defines result of speech recognition.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs">
            <summary>
            Define payload of speech recognizing/recognized events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs.Result">
            <summary>
            Specifies the recognition result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs">
            <summary>
            Define payload of speech recognition canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in version 1.1.0.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResultExtensions">
            <summary>
            Extension methods for speech recognition result
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognitionResultExtensions.Best(Microsoft.CognitiveServices.Speech.SpeechRecognitionResult)">
            <summary>
            Returns best possible recognitions for the result if the recognizer
            was created with detailed output format.
            </summary>
            <param name="result">Recognition result.</param>
            <returns>A collection of best recognitions.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognizer">
             <summary>
             Performs speech recognition from microphone, file, or other audio input streams, and gets transcribed text as result.
             </summary>
             <example>
             An example to use the speech recognizer from microphone and listen to events generated by the recognizer.
             <code>
             public async Task SpeechContinuousRecognitionAsync()
             {
                 // Creates an instance of a speech config with specified subscription key and service region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
            
                 // Creates a speech recognizer from microphone.
                 using (var recognizer = new SpeechRecognizer(config))
                 {
                     // Subscribes to events.
                     recognizer.Recognizing += (s, e) => {
                         Console.WriteLine($"RECOGNIZING: Text={e.Result.Text}");
                     };
            
                     recognizer.Recognized += (s, e) => {
                         var result = e.Result;
                         Console.WriteLine($"Reason: {result.Reason.ToString()}");
                         if (result.Reason == ResultReason.RecognizedSpeech)
                         {
                                 Console.WriteLine($"Final result: Text: {result.Text}.");
                         }
                     };
            
                     recognizer.Canceled += (s, e) => {
                         Console.WriteLine($"\n    Recognition Canceled. Reason: {e.Reason.ToString()}, CanceledReason: {e.Reason}");
                     };
            
                     recognizer.SessionStarted += (s, e) => {
                         Console.WriteLine("\n    Session started event.");
                     };
            
                     recognizer.SessionStopped += (s, e) => {
                         Console.WriteLine("\n    Session stopped event.");
                     };
            
                     // Starts continuous recognition. Uses StopContinuousRecognitionAsync() to stop recognition.
                     await recognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);
            
                     do
                     {
                         Console.WriteLine("Press Enter to stop");
                     } while (Console.ReadKey().Key != ConsoleKey.Enter);
            
                     // Stops recognition.
                     await recognizer.StopContinuousRecognitionAsync().ConfigureAwait(false);
                 }
             }
             </code>
             </example>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognizing"/> signals that an intermediate recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Canceled"/> signals that the speech recognition was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.EndpointId">
            <summary>
            Gets the endpoint ID of a customized speech model that is used for speech recognition.
            </summary>
            <returns>the endpoint ID of a customized speech model that is used for speech recognition</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.AuthorizationToken">
            <summary>
            Gets/sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that was set when the recognizer was created.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognizer"/>.
            Note: The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.RecognizeOnceAsync">
             <summary>
             Starts speech recognition, and returns after a single utterance is recognized. The end of a
             single utterance is determined by listening for silence at the end or until a maximum of 15
             seconds of audio is processed.  The task returns the recognition text as result.
             Note: Since RecognizeOnceAsync() returns only a single utterance, it is suitable only for single
             shot recognition like command or query.
             For long-running multi-utterance recognition, use StartContinuousRecognitionAsync() instead.
             </summary>
             <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResult"/> </returns>
             <example>
             The following example creates a speech recognizer, and then gets and prints the recognition result.
             <code>
             public async Task SpeechSingleShotRecognitionAsync()
             {
                 // Creates an instance of a speech config with specified subscription key and service region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
            
                 // Creates a speech recognizer using microphone as audio input. The default language is "en-us".
                 using (var recognizer = new SpeechRecognizer(config))
                 {
                     Console.WriteLine("Say something...");
            
                     // Starts speech recognition, and returns after a single utterance is recognized. The end of a
                     // single utterance is determined by listening for silence at the end or until a maximum of 15
                     // seconds of audio is processed.  The task returns the recognition text as result.
                     // Note: Since RecognizeOnceAsync() returns only a single utterance, it is suitable only for single
                     // shot recognition like command or query.
                     // For long-running multi-utterance recognition, use StartContinuousRecognitionAsync() instead.
                     var result = await recognizer.RecognizeOnceAsync();
            
                     // Checks result.
                     if (result.Reason == ResultReason.RecognizedSpeech)
                     {
                         Console.WriteLine($"RECOGNIZED: Text={result.Text}");
                     }
                     else if (result.Reason == ResultReason.NoMatch)
                     {
                         Console.WriteLine($"NOMATCH: Speech could not be recognized.");
                     }
                     else if (result.Reason == ResultReason.Canceled)
                     {
                         var cancellation = CancellationDetails.FromResult(result);
                         Console.WriteLine($"CANCELED: Reason={cancellation.Reason}");
            
                         if (cancellation.Reason == CancellationReason.Error)
                         {
                             Console.WriteLine($"CANCELED: ErrorCode={cancelation.ErrorCode}");
                             Console.WriteLine($"CANCELED: ErrorDetails={cancellation.ErrorDetails}");
                             Console.WriteLine($"CANCELED: Did you update the subscription info?");
                         }
                     }
                 }
             }
             </code>
             </example>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StartContinuousRecognitionAsync">
            <summary>
            Starts speech recognition on a continuous audio stream, until StopContinuousRecognitionAsync() is called.
            User must subscribe to events to receive recognition results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StopContinuousRecognitionAsync">
            <summary>
            Stops continuous speech recognition.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Starts speech recognition on a continuous audio stream with keyword spotting, until StopKeywordRecognitionAsync() is called.
            User must subscribe to events to receive recognition results.
            </summary>
            Note: Keyword spotting (KWS) functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.
            <param name="model">The keyword recognition model that specifies the keyword to be recognized.</param>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StopKeywordRecognitionAsync">
            <summary>
            Stops continuous speech recognition with keyword spotting.
            </summary>
            Note: Keyword spotting (KWS) functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat">
            <summary>
            Defines the possible speech synthesis output audio format.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw8Khz8BitMonoMULaw">
            <summary>
            raw-8khz-8bit-mono-mulaw
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff16Khz16KbpsMonoSiren">
            <summary>
            riff-16khz-16kbps-mono-siren
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz16KbpsMonoSiren">
            <summary>
            audio-16khz-16kbps-mono-siren
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3">
            <summary>
            audio-16khz-32kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz128KBitRateMonoMp3">
            <summary>
            audio-16khz-128kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz64KBitRateMonoMp3">
            <summary>
            audio-16khz-64kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz48KBitRateMonoMp3">
            <summary>
            audio-24khz-48kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz96KBitRateMonoMp3">
            <summary>
            audio-24khz-96kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz160KBitRateMonoMp3">
            <summary>
            audio-24khz-160kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw16Khz16BitMonoTrueSilk">
            <summary>
            raw-16khz-16bit-mono-truesilk
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff16Khz16BitMonoPcm">
            <summary>
            riff-16khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff8Khz16BitMonoPcm">
            <summary>
            riff-8khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm">
            <summary>
            riff-24khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff8Khz8BitMonoMULaw">
            <summary>
            riff-8khz-8bit-mono-mulaw
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw16Khz16BitMonoPcm">
            <summary>
            raw-16khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm">
            <summary>
            raw-24khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw8Khz16BitMonoPcm">
            <summary>
            raw-8khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult">
            <summary>
            Contains detailed information about result of a speech synthesis operation.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.ResultId">
            <summary>
            Specifies unique ID of speech synthesis result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.Reason">
            <summary>
            Specifies status of speech synthesis result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.AudioData">
            <summary>
            Presents the synthesized audio in the result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails">
            <summary>
            Contains detailed information about why a speech synthesis result was canceled.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.SpeechSynthesisResult)">
            <summary>
            Creates an instance of SpeechSynthesisCancellationDetails object for the canceled SpeechSynthesisResult.
            </summary>
            <param name="result">The result that was canceled.</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.FromStream(Microsoft.CognitiveServices.Speech.AudioDataStream)">
            <summary>
            Creates an instance of SpeechSynthesisCancellationDetails object for the canceled AudioDataStream.
            </summary>
            <param name="stream">The stream that was canceled</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.Reason">
            <summary>
            The reason the synthesis was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.ErrorCode">
            <summary>
            The error code in case of an unsuccessful synthesis (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful synthesis (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisEventArgs">
            <summary>
            Define payload of speech synthesis events.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisEventArgs.Result">
            <summary>
            Specifies the synthesis result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisEventArgs.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesizer">
            <summary>
            Performs speech synthesis to speaker, file, or other audio output streams, and gets synthesized audio as result.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisStarted">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisStarted"/> signals that the speech synthesis has started.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.Synthesizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.Synthesizing"/> signals that the speech synthesis is on going.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisCompleted">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisCompleted"/> signals that the speech synthesis has completed.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisCanceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisCanceled"/> signals that the speech synthesis was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.SpeechSynthesizer"/>.
            Note: The property collection is only valid until the SpeechSynthesizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SpeakTextAsync(System.String)">
            <summary>
            Execute the speech synthesis on plain text, asynchronously.
            </summary>
            <param name="text">The plain text for synthesis.</param>
            <returns>A task representing the synthesis operation. The task returns a value of SpeechSynthesisResult.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SpeakSsmlAsync(System.String)">
            <summary>
            Execute the speech synthesis on SSML, asynchronously.
            </summary>
            <param name="ssml">The SSML for synthesis.</param>
            <returns>A task representing the synthesis operation. The task returns a value of SpeechSynthesisResult.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.StartSpeakingTextAsync(System.String)">
            <summary>
            Start the speech synthesis on plain text, asynchronously.
            </summary>
            <param name="text">The plain text for synthesis.</param>
            <returns>A task representing the synthesis operation. The task returns a value of SpeechSynthesisResult.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.StartSpeakingSsmlAsync(System.String)">
            <summary>
            Start the speech synthesis on SSML, asynchronously.
            </summary>
            <param name="ssml">The SSML for synthesis.</param>
            <returns>A task representing the synthesis operation. The task returns a value of SpeechSynthesisResult.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig">
            <summary>
            Speech translation configuration.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromSubscription(System.String,System.String)">
            <summary>
            Creates an instance of speech translation config with specified subscription key and region.
            </summary>
            <param name="subscriptionKey">The subscription key, can be empty if authorization token is specified later.</param>
            <param name="region">The region name (see the <a href="https://aka.ms/csspeech/region">region page</a>).</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromAuthorizationToken(System.String,System.String)">
            <summary>
            Creates an instance of the speech translation config with specified authorization token and region.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter on the corresponding
            recognizer with a new valid token.
            </summary>
            <param name="authorizationToken">The authorization token.</param>
            <param name="region">The region name (see the <a href="https://aka.ms/csspeech/region">region page</a>).</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromEndpoint(System.Uri,System.String)">
            <summary>
            Creates an instance of the speech translation config with specified endpoint and subscription key.
            This method is intended only for users who use a non-standard service endpoint or parameters.
            Note: The query parameters specified in the endpoint URI are not changed, even if they are set by any other APIs.
            For example, if the recognition language is defined in URI as query parameter "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US",
            the language setting in URI takes precedence, and the effective language is "de-DE".
            Only the parameters that are not specified in the endpoint URI can be set by other APIs.
            Note: To use an authorization token with FromEndpoint, use FromEndpoint(System.Uri),
            and then set the AuthorizationToken property on the created SpeechTranslationConfig instance.
            </summary>
            <param name="endpoint">The service endpoint to connect to.</param>
            <param name="subscriptionKey">The subscription key.</param>
            <returns>A SpeechTranslationConfig instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromEndpoint(System.Uri)">
            <summary>
            Creates an instance of the speech translation config with specified endpoint.
            This method is intended only for users who use a non-standard service endpoint or parameters.
            Note: The query parameters specified in the endpoint URI are not changed, even if they are set by any other APIs.
            For example, if the recognition language is defined in URI as query parameter "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US",
            the language setting in URI takes precedence, and the effective language is "de-DE".
            Only the parameters that are not specified in the endpoint URI can be set by other APIs.
            Note: If the endpoint requires a subscription key for authentication, use FromEndpoint(System.Uri, string) to pass
            the subscription key as parameter.
            To use an authorization token with FromEndpoint, please use this method to create a SpeechTranslationConfig instance, and then
            set the AuthorizationToken property on the created SpeechTranslationConfig instance.
            Note: Added in version 1.5.0.
            </summary>
            <param name="endpoint">The service endpoint to connect to.</param>
            <returns>A SpeechTranslationConfig instance.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.TargetLanguages">
            <summary>
            Gets a collection of languages to translate to.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.AddTargetLanguage(System.String)">
            <summary>
            Add a target languages of translation.
            In case when speech synthesis is used and several target languages are specified for translation,
            the speech will be synthesized only for the first language.
            </summary>
            <param name="language"></param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.VoiceName">
            <summary>
            Specifies the name of voice tag if a synthesized audio output is desired.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.StreamStatus">
            <summary>
            Defines the possible status of audio data stream.
            Added in version 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.Unknown">
            <summary>
            The audio data stream status is unknown
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.NoData">
            <summary>
            The audio data stream contains no data
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.PartialData">
            <summary>
            The audio data stream contains partial data of a speak request
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.AllData">
            <summary>
            The audio data stream contains all data of a speak request
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.Canceled">
            <summary>
            The audio data stream was cancelled
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult">
            <summary>
            Defines tranlsation result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult.Translations">
            <summary>
            Presents the translation results. Each item in the dictionary represents translation result in one of target languages, where the key
            is the name of the target language, in BCP-47 format, and the value is the translation text in the specified language.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult.ToString">
            <summary>
            Returns a string that represents the speech recognition result.
            </summary>
            <returns>A string that represents the speech recognition result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer">
             <summary>
             Performs translation on the speech input.
             </summary>
             <example>
             An example to use the translation recognizer from microphone and listen to events generated by the recognizer.
             <code>
             public async Task TranslationContinuousRecognitionAsync()
             {
                 // Creates an instance of a speech translation config with specified subscription key and service region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechTranslationConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
            
                 // Sets source and target languages.
                 string fromLanguage = "en-US";
                 config.SpeechRecognitionLanguage = fromLanguage;
                 config.AddTargetLanguage("de");
            
                 // Sets voice name of synthesis output.
                 const string GermanVoice = "Microsoft Server Speech Text to Speech Voice (de-DE, Hedda)";
                 config.VoiceName = GermanVoice;
                 // Creates a translation recognizer using microphone as audio input.
                 using (var recognizer = new TranslationRecognizer(config))
                 {
                     // Subscribes to events.
                     recognizer.Recognizing += (s, e) =>
                     {
                         Console.WriteLine($"RECOGNIZING in '{fromLanguage}': Text={e.Result.Text}");
                         foreach (var element in e.Result.Translations)
                         {
                             Console.WriteLine($"    TRANSLATING into '{element.Key}': {element.Value}");
                         }
                     };
            
                     recognizer.Recognized += (s, e) =>
                     {
                         if (e.Result.Reason == ResultReason.TranslatedSpeech)
                         {
                             Console.WriteLine($"\nFinal result: Reason: {e.Result.Reason.ToString()}, recognized text in {fromLanguage}: {e.Result.Text}.");
                             foreach (var element in e.Result.Translations)
                             {
                                 Console.WriteLine($"    TRANSLATING into '{element.Key}': {element.Value}");
                             }
                         }
                     };
            
                     recognizer.Synthesizing += (s, e) =>
                     {
                         var audio = e.Result.GetAudio();
                         Console.WriteLine(audio.Length != 0
                             ? $"AudioSize: {audio.Length}"
                             : $"AudioSize: {audio.Length} (end of synthesis data)");
                     };
            
                     recognizer.Canceled += (s, e) =>
                     {
                         Console.WriteLine($"\nRecognition canceled. Reason: {e.Reason}; ErrorDetails: {e.ErrorDetails}");
                     };
            
                     recognizer.SessionStarted += (s, e) =>
                     {
                         Console.WriteLine("\nSession started event.");
                     };
            
                     recognizer.SessionStopped += (s, e) =>
                     {
                         Console.WriteLine("\nSession stopped event.");
                     };
            
                     // Starts continuous recognition. Uses StopContinuousRecognitionAsync() to stop recognition.
                     Console.WriteLine("Say something...");
                     await recognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);
            
                     do
                     {
                         Console.WriteLine("Press Enter to stop");
                     } while (Console.ReadKey().Key != ConsoleKey.Enter);
            
                     // Stops continuous recognition.
                     await recognizer.StopContinuousRecognitionAsync().ConfigureAwait(false);
                 }
             }
             </code>
             </example>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognizing"/> signals that an intermediate recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Canceled"/> signals that the speech to text/synthesis translation was canceled.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Synthesizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Synthesizing"/> signals that a translation synthesis result is received.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechTranslationConfig)">
            <summary>
            Creates a translation recognizer using the default microphone input for a specified translation configuration.
            </summary>
            <param name="config">Translation config.</param>
            <returns>A translation recognizer instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechTranslationConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a translation recognizer using the specified speech translator and audio configuration.
            </summary>
            <param name="config">Translation config.</param>
            <param name="audioConfig">Audio config.</param>
            <returns>A translation recognizer instance.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that was set when the recognizer was created.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.TargetLanguages">
            <summary>
            Gets target languages for translation that were set when the recognizer was created.
            The language is specified in BCP-47 format. The translation will provide translated text for each of language.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.VoiceName">
            <summary>
            Gets the name of output voice if speech synthesis is used.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer"/>.
            Note: The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.AuthorizationToken">
            <summary>
            Gets/sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.RecognizeOnceAsync">
             <summary>
             Starts speech translation, and returns after a single utterance is recognized. The end of a
             single utterance is determined by listening for silence at the end or until a maximum of 15
             seconds of audio is processed.  The task returns the recognition text as result.
             Note: Since RecognizeOnceAsync() returns only a single utterance, it is suitable only for single
             shot recognition like command or query.
             For long-running multi-utterance recognition, use StartContinuousRecognitionAsync() instead.
             </summary>
             <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult"/> </returns>
             <example>
             Create a translation recognizer, get and print the recognition result
             <code>
             public async Task TranslationSingleShotRecognitionAsync()
             {
                 // Creates an instance of a speech translation config with specified subscription key and service region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechTranslationConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
            
                 string fromLanguage = "en-US";
                 config.SpeechRecognitionLanguage = fromLanguage;
                 config.AddTargetLanguage("de");
            
                 // Creates a translation recognizer.
                 using (var recognizer = new TranslationRecognizer(config))
                 {
                     // Starts recognizing.
                     Console.WriteLine("Say something...");
            
                     // Starts translation recognition, and returns after a single utterance is recognized. The end of a
                     // single utterance is determined by listening for silence at the end or until a maximum of 15
                     // seconds of audio is processed. The task returns the recognized text as well as the translation.
                     // Note: Since RecognizeOnceAsync() returns only a single utterance, it is suitable only for single
                     // shot recognition like command or query.
                     // For long-running multi-utterance recognition, use StartContinuousRecognitionAsync() instead.
                     var result = await recognizer.RecognizeOnceAsync();
            
                     if (result.Reason == ResultReason.TranslatedSpeech)
                     {
                         Console.WriteLine($"\nFinal result: Reason: {result.Reason.ToString()}, recognized text: {result.Text}.");
                         foreach (var element in result.Translations)
                         {
                             Console.WriteLine($"    TRANSLATING into '{element.Key}': {element.Value}");
                         }
                     }
                 }
             }
             </code>
             </example>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StartContinuousRecognitionAsync">
            <summary>
            Starts recognition and translation on a continous audio stream, until StopContinuousRecognitionAsync() is called.
            User must subscribe to events to receive translation results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StopContinuousRecognitionAsync">
            <summary>
            Stops continuous recognition and translation.
            </summary>
            <returns>A task representing the asynchronous operation that stops the translation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Starts speech recognition on a continuous audio stream with keyword spotting, until StopKeywordRecognitionAsync() is called.
            User must subscribe to events to receive recognition results.
            </summary>
            Note: Keyword spotting (KWS) functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.
            <param name="model">The keyword recognition model that specifies the keyword to be recognized.</param>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StopKeywordRecognitionAsync">
            <summary>
            Stops continuous speech recognition with keyword spotting.
            </summary>
            Note: Keyword spotting (KWS) functionality might work with any microphone type, official KWS support, however, is currently limited to the microphone arrays found in the Azure Kinect DK hardware or the Speech Devices SDK.
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult">
            <summary>
            Defines translation synthesis result, i.e. the voice output of the translated text in the target language.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult.Reason">
            <summary>
            Indicates the possible reasons a TranslationSynthesisResult might be generated.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult.GetAudio">
            <summary>
            The voice output of the translated text in the target language.
            </summary>
            <returns>Synthesized audio data.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult.ToString">
            <summary>
            Returns a string that represents the synthesis result.
            </summary>
            <returns>A string that represents the synthesis result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisEventArgs">
            <summary>
            Define payload of translation synthesis result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisEventArgs.Result">
            <summary>
            Specifies the translation synthesis result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionEventArgs">
            <summary>
            Define payload of translation recognizing/recognized events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionEventArgs.Result">
            <summary>
            Specifies the recognition result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs">
            <summary>
            Define payload of translation text result recognition canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in version 1.1.0.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
    </members>
</doc>
